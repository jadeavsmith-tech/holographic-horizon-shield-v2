import ollama
import re
import yaml
import os

# Load config if exists
CONFIG_FILE = "config.yaml"
BAD_PATTERNS = [
    r"ignore\s*previous\s*instructions",
    r"dan\s*mode",
    r"jailbreak",
    r"system\s*prompt",
    r"do\s*anything\s*now",
    r"role\s*play\s*as\s*(?!safe)",
]  # Expandable

if os.path.exists(CONFIG_FILE):
    with open(CONFIG_FILE) as f:
        config = yaml.safe_load(f) or {}
        BAD_PATTERNS = config.get("bad_patterns", BAD_PATTERNS)

def outer_boundary_scan(prompt: str) -> tuple[bool, str]:
    lower = prompt.lower()
    for pattern in BAD_PATTERNS:
        if re.search(pattern, lower):
            return True, f"Forbidden pattern matched: {pattern}"
    return False, "Passed outer boundary"

def phi3_guard(prompt: str) -> tuple[str, str]:
    try:
        response = ollama.chat(
            model='phi3',
            messages=[
                {'role': 'system', 'content': 'You are Horizon Guard. Classify prompt as SAFE or MALICIOUS. Respond ONLY: SAFE/MALICIOUS + one-sentence reason.'},
                {'role': 'user', 'content': prompt}
            ]
        )
        text = response['message']['content'].upper()
        verdict = "MALICIOUS" if "MALICIOUS" in text else "SAFE"
        reason = response['message']['content'].strip()
        return verdict, reason
    except Exception as e:
        return "SAFE", f"Guard error ({e})"

def full_scan(prompt: str) -> dict:
    blocked, reason = outer_boundary_scan(prompt)
    if blocked:
        return {"verdict": "BLOCKED", "layer": "Outer Boundary", "reason": reason}

    verdict, reason = phi3_guard(prompt)
    if verdict == "MALICIOUS":
        return {"verdict": "BLOCKED", "layer": "Phi-3 Guard", "reason": reason}

    return {"verdict": "SAFE", "layer": "All Layers", "reason": "No threats detected"}
